---
title: "Non-Parametric Statistics in R"
author: "Adam J Sullivan, PhD"
date: "1/17/2019"
output:
  ioslides_presentation: 
    widescreen: true
  beamer_presentation: default
notes: ''
link: yes
reading:  <a href="https://php-1511-2511.github.io/Introduction-to-R/">Introduction to R </a>
slides: yes
layout: page
css:  "D:/Dropbox/Dropbox (Personal)/Brown/Teaching/Brown Courses/PHP2511/Spring 2018/website/php-1511-2511.github.io/Notes/slides.css"
---




```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

# Non Parametric Statistics



## What are they?

- Non parametrics means that you do not need to specify a specific distribution for the data. 
- Many of the methods you have learned up to this point require data dealing with the normal distribution. 
- This is due partially to the fact that the t-distribution, $\chi^2$ distribution, and the $F$ distribution can all be derived from the normal distribution. 
- Traditionally you are just taught to use normallity and made to either transform the data or just go ahead knowing it is incorrect. 




## Normal vs Skewed Data 

- Data that is normally distributed has:
    - the mean and the median the same.
    - The data is centered about the mean.
    - Very specific probability values.
- Data that is skewed has: 
    - Mean less than median for left skewed data.
    - Mean greater than median for right skewed data. 





## Why is this an issue?

- In 1998 a survey was given to Harvard students who entered in 1973:
  - The mean salary was $750,000
  - The median salary was $175,000
- What could be a problem with this?
- What happened here? 



## Why do we use Parametric Models?

1. Parametric Models have more power so can more easily detect significant differences. 
2. Given large sample size Parametric models perform well even in non-normal data. 
3. Central limit theorem states that in research that can be perfromed over and over again, that the means are normally distributed. 
4. There are methods to deal with incorrect variances. 




## Why do we use Non Parametric Models?

1. Your data is better represented by the median. 
2. You have small sample sizes. 
3. You cannot see the ability to replicate this work. 
4. You have ordinal data, ranked data, or outliers that you canâ€™t remove.




## What Non Parametric Tests will we cover? 

- Sign Test
- Wilcoxon Signed-Rank Test
- Wilcoxon Rank-Sum Test (Mann-Whitney U Test, ...)
- Kruskal Wallis test
- Spearman Rank Correlation Coefficient
- Bootstrapping





# The sign Test



## The Sign Test

- The sign test can be used when comparing 2 samples of observarions when there is not independence of samples. 
- It actually does compares matches together in order to accomplish its task. 
- This is similar to the paired t-test
- No need for the assumption of normality. 
- Uses the Binomial Distribution




## Steps of the Sign Test

- We first match the data 
- Then we subtract the 2nd value from the 1st value. 
- You then look at the sign of each subtraction. 
- If there is no difference between the two groups you shoul have roughly 50% positives and 50% negatives. 
- Compare the proportion of positives you have to a binomial with p=0.5. 



## Example: Binomial Test Function

- Consider the scenario where you have patients with Cystic Fibrosis and health individuals. 
- Each subject with CF has been matched to a healthy individual on age, sex, height and weight. 
- We will compare the Resting Energy Expenditure (kcal/day)




## Reading in the Data

```{r, error=F}
library(readr)
ree <- read_csv("../data/ree.csv")
ree
```





## Function in R

- Comes from the `BDSA` Package

```
SIGN.test(x ,y, md=0, alternative = "two.sidesd",
conf.level=0.95)
```

- Where 
  - `x` is a vector of values
  - `y` is an optional vector of values.
  - `md` is median and defaults to 0. 
  - `alternative` is way to specific type of test.
  - `conf.level` specifies $1-\alpha$.



## Getting Package

```{r}

# install.packages("devtools")
# devtools::install_github('alanarnholt/BSDA')
```




## Our Data

```{r, eval=FALSE}
library(BSDA)
attach(ree)
SIGN.test(CF, Healthy)
detach()
```



## Our Data

```{r, echo=FALSE}
library(BSDA)
attach(ree)
SIGN.test(CF, Healthy)
detach()
```



## By "Hand"

- Subtract Values

```{r, eval=FALSE}
library(tidyverse)
ree <- ree %>%
mutate(diff = CF - Healthy)
ree
```




## By "Hand"


- count negatives

```{r, echo=FALSE}
library(tidyverse)
ree <- ree %>%
mutate(diff = CF - Healthy)
ree
```



## By "Hand"

```{r}
binom.test(2,13)
```



# Wilcoxon Signed-Rank Test



## Wilcoxon Signed-Rank Test

- The sign test works well but it truly ignores the magntiude of the differences. 
- Sign test often not used due to this problem. 
- Wilcoxon Signed Rank takes into account both the sign and the rank




## How does it work? 

- Pairs the data based on study design. 
- Subtracts data just like the sign test.
- Ranks the magnitude of the difference:

```
8
-17
52
-76
```




## What happens with these ranks?

| Subtraction | Positive Ranks | Negative Ranks |
| ----------- | -------------- | -------------- | 
| 8  | 1 |  | 
| -17 |  | -2 | 
| 52 | 3 |  | 
| -76 |  | -4 | 
**Sum** | **4** | **-6** |





## What about after the sum?

- $W_{+}= 1 + 3 = 4$
- $W_{-} + -2 + -4 = -6$
- Mean: $\dfrac{n(n+1)}{4}$
- Variance: $\dfrac{n(n+1)(2n+1)}{24}$



## What about after the sum?

- Any ties, $t$: decrease variance by $t^3-\dfrac{t}{48}$
- z test:

$$ z =\dfrac{W_{smaller}- \dfrac{n(n+1)}{4}}{\sqrt{\dfrac{n(n+1)(2n+1)}{24}-t^3-\dfrac{t}{48}}}$$




## Wilcoxon Signed Rank in R

```{r}
attach(ree)
wilcox.test(CF, Healthy, paired=T)
detach(ree)
```






# Wilcoxon Rank-Sum Test



## Wilcoxon Rank-Sum Test

- This test is used on indepdent data. 
- It is the non-parametric version of the 2-sample $t$-test.
- Does not requre normality or equal variance. 




## How do we do it?

- Order each sample from least to greatest
- Rank them. 
- Sum the ranks of each sample




## What do we do with summed ranks?

- $W_s$ smaller of 2 sums.
- Mean: $\dfrac{n_s(n_s+n_L + 1)}{2}$
- Variance: $\dfrac{n_sn_L(n_s+n_L+1)}{12}$
- z-test

$$ z = \dfrac{W_s-\dfrac{n_s(n_s+n_L + 1)}{2}}{\sqrt{\dfrac{n_sn_L(n_s+n_L+1)}{12}}}$$






## Wilcoxon Rank-Sum in R

- Consider built in data `mtcars`

```{r}
library(tidyverse)
cars <- as_data_frame(mtcars)
cars
```




## Wilcoxon Rank-Sum in R

- We will Consider `mpg` and `am`
- `mpg`: Miles Per Gallon on Average
- `am`
    - 0: automatic transmission
    - 1: manual transmission

 

## Wilcoxon Rank-Sum in R

```{r}
attach(cars)
wilcox.test(mpg, am)
detach(cars)
```




# Kruskal Wallis Test



## Kruskal Wallis Test

- If we have multiple groups of independent data that are not normally distributed or have variance issues, you can use the Kruskal Wallis Test.
- It tests significant differences in medians of the groups. 
- This is a non-parametric method for One-Way ANOVA.
- Harder to try and calculate by hand, so we will just use R.



## Kruskal Wallis Test in R

```
kruskal.test(formula, data, subset, ...)
```

- Where
    - `formula` is `y~x` or can enter `outcome,group` instead.
    - `data` is the dataframe of interest.
    - `subset` if you wish to test subset of data. 



## Arthritis Data

- comes from the `BSDA` package. 
- `Arthriti`

| Variable | Description | 
| -------- | ----------- | 
| `time` | Time in Days until patient felt relief | 
| `treatment` | Factor with three levels `A`, `B`, and `C` |




## Arthritis Data

```{r}
library(BSDA)
Arthriti
```





## Kruskal-Wallis Test in R

```{r}
kruskal.test(time~treatment, data=Arthriti)
```








#  Spearman Rank Correlation Coefficient



##  Spearman Rank Correlation Coefficient


- Correlation is a measurement of the strength of a linear relationship between variables. 
- This means it does not necessarily get the actual magnitude of relationship. 
- Spearman Rank Correlation seeks to fix this. 
- It works with Montonic Data.

 

## Monotonic Data

```{r, echo=F}
library(ggplot2)
library(gridExtra)

x1 <- seq(1,100, length=1000)
x2 <- seq(1,5, length=1000)

y1 <- (1/4) * x1^2 + rnorm(1000,0,1) # Monotonically Increasing Function
y2 <- exp(-x2+ rnorm(1000,0,0.1)) # Montonically Decreasing
y3 <- sin(x1 / 5) + rnorm(1000,0,1) # Not Monotonic

df <- as_data_frame(cbind(x1,x2,y1,y2,y3))


p1 <- ggplot(df, aes(x1,y1)) +
  geom_point() + 
  ggtitle("Monotonic Increasing")
  
p2 <- ggplot(df, aes(x2,y2)) +
  geom_point() + 
  ggtitle("Monotonic Decreasing")
  
p3 <- ggplot(df, aes(x1,y3)) +
  geom_point() + 
  ggtitle("Non Monotonic")


grid.arrange(p1,p2,p3, nrow=1)
```




## Spearman Rank Correlation in R

- We can do this the the `cor()` function. 

```{r}
#Pearson from Monotonic Decreasing
cor(x2,y2, method="pearson")

#Spearman from Monotonic Decreasing
cor(x2,y2, method="spearman")
```




#  Bootstrapping



##  Bootstrapping


- Linear regression itself works well even with non-normal errors.
- What can happen with non-normal errors is that our standard error is estimated incorrectly and therefore we cannot trust our p-values or confidence intervals. 
- Bootstrap is a non-parametric technique to get an idea of variance. 




## Bootstrapping

- Treats your data as a population. 
- Draws samples from the data with replacement. 
- Steps:
    1. So if you have 100 subjects, you draw a sample of size 100 from all 100 subjects, some may get picked twice or more and others not at all. 
    2. Then run your regression over this dataset.
    3. Pull out coefficients.
    4. Repeat steps 1-3 say 1000 times. 
    5. Get 95% interval from now normal data. 





## Bootstrapping in R

- Use the `boot` package.

```
boot(data, statistic, R, ....)
```

- Where
    - `data` is your dataframe of interest.
    - `statistic` is what you want to bootstrap
    - `R` how many replicates do you want. 
    - `...` other options from `statistic` or `boot` function. 




## Bootstrapping Regression Coefficients

```{r, eval=F}
# Bootstrap 95% CI for regression coefficients 
library(boot)
# function to obtain regression weights 
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  return(coef(fit)) 
} 
# bootstrapping with 1000 replications 
results <- boot(data=mtcars, statistic=bs, 
  	R=1000, formula=mpg~wt+disp)

# view results
results
plot(results, index=1) # intercept 
plot(results, index=2) # wt 
plot(results, index=3) # disp 

# get 95% confidence intervals 
boot.ci(results, type="bca", index=1) # intercept 
boot.ci(results, type="bca", index=2) # wt 
boot.ci(results, type="bca", index=3) # disp
```




## Bootstraping $R^2$

```{r, eval=F}
# Bootstrap 95% CI for R-Squared
library(boot)
# function to obtain R-Squared from the data 
rsq <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  return(summary(fit)$r.square)
} 
# bootstrapping with 1000 replications 
results <- boot(data=mtcars, statistic=rsq, 
  	R=1000, formula=mpg~wt+disp)

# view results
results 
plot(results)

# get 95% confidence interval 
boot.ci(results, type="bca")
```





#  Other Methods



##  Other Methods

- There are various other Methods that we do not have the ability to really discuss here
    - Generalized Additive Models
    - Splines and Other penalized Regressions
    - Classification and Regression Trees
    - Smoothing Regressions
    - Permutation Tests  

